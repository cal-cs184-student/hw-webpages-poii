<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Christopher Hoang, Walter Wu</div>

		<br>

		Link to webpage:<a href="https://cal-cs184-student.github.io/hw-webpages-poii/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-poii/hw3/index.html</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-colorriseharmony">https://github.com/cal-cs184-student/sp25-hw3-colorriseharmony</a>
		
		

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		<p>
			In this assignment, we created a ray tracer by implementing some techniques that are used to simulate illumination. Specifically, we implemented ray generation in scenes, ways to check object (triangle and sphere) intersection with the rays, bounding volume hierarchies (BVHs) and acceleration structures, ways to check if our rays intersected these data structures, ways to simulate direct and global illumination, and adaptive sampling. Working on ray generation first gave us a good understanding of the different coordinate systems we would need to work with throughout the assignment, and utilizing acceleration structures really enabled us to test our code efficiently. In fact, I remember being visibly shocked at how much our rendering speed improved after its implementation. From there, we were able to try implementing different illumination techniques that helped us build intuition for how light interacts with an environment. In fact, working through the calculations and the sampling involved with light simulation has started to make us think about light differently in the real world as well. It's just so fascinating how an image produced by rendering only the direct lighting can actually technically be found in the real world, but light is just too fast for us to detect it in this state. Overall, a lot of the techniques and high-level ideas involved with this assignment were straightforward, but the sheer number of places where a bug from n-parts ago could pop up was extremely frustrating to deal with. For example, you could implement something incorrectly, but that produces the correct results at the time, and then not detect a bug resulting from that implementation until many parts after. On top of that, some images took up to an hour to render on our machines, so sometimes we had to wait extremely long to see if we had fixed a bug. Anyways, we got through it, and it felt very satisfying to complete it in the end!
		</p>

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<p>
			In the rendering pipeline, we need to essentially create rays of light and then track how they bounce around and interact with a scene to generate our desired rendering of it. This is done with ray generation and keeping track of what primitives are intersected by each ray. Once we understand which rays of light will bounce/interact with which objects in our scene, we can render our scene accordingly.
		</p>
		<p>
			To generate rays, we need to translate from image space to camera space to world space. First, we are given normalized image coordinates that we transform into camera space, and once we are in camera space, we can generate a ray that points from our camera to our transformed coordinate. Once we have this ray, we can transform the ray into world space. 
		</p>
		<p>
			Then, to check primitive intersection, we used different equations depending on the primitives we wanted to check, but they each followed the same procedure of defining our ray and primitive's shape, using math to see if our ray (which is a line) intersects the primitive at any point based on a parameter t (time), and then deciding whether or not the time at which our ray intersects the primitive is appropriate for our simulation.
		</p>
		<p>
			For specifics on our ray generation's implementation, we followed the provided diagram's dimensions to create a 3D vector that contained our transformed x and y coordinates while adding a z coordinate that was placed at z = -1, which is where the sensor is placed in camera space. Now that we had our point's coordinates in camera space, we created a ray vector that points from the camera origin, (0, 0, 0), to our desired coordinate on the sensor by using vector subtraction. Now that we had our ray vector in camera space, all we had to do was multiply it by the provided c2w transformation matrix to get our desired ray in world space. Using our ray and the camera position in world space, we generated our final ray object and initialized it with min_t and max_t equal to nClip and fClip respectively.
		</p>
		<p>
			Then for specifics on our triangle intersection algorithm's implementation, we used the Mollor Trumbore Algorithm. Given our ray and triangle definition, the Mollor Trumbor Algorithm will generate a vector that contains the time of our ray's intersection with a triangle, as well as the barycentric coordinates of our ray's intersection point in relation to the triangle's points. Once we had the final resulting vector, we checked to see if the resulting time was appropriate (fell within the ray's time bounds) and then if the barycentric coordinates indicated that the point lay inside the defined triangle (there were no negative barycentric coordinate values). If the Mollor Trumbore Algorithm indicated that our ray appropriately intersected the desired triangle, then we redefined the ray's max_t member variable to the newly found time of intersection and also updated the provided isect Intersection object accordingly.
		</p>

		<p>Here are some images of small dae files with normal shading.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part1lambspheres.png" width="400px"/>
				  <figcaption>Lambertian Spheres Image</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part1teapot.png" width="400px"/>
				  <figcaption>Utah Teapot Image</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part1gems.png" width="400px"/>
				  <figcaption>CBGems Image</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part1cow.png" width="400px"/>
				  <figcaption>Cow Image</figcaption>
				</td>
			  </tr>
			</table>
		</div>


		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>
			In our Bounding Volume Hierarchy (BVH) construction algorithm, we first started with constructing a 
			bounding box that contained the bounding boxes of all of our desired primitives and set it as a new 
			BVH node. Then, we used recursion to split this current node's bounding box into more BVH nodes.
		</p>

		<p>
			The base (termination) case for our implementation is that if the current number of primitives 
			passed into the method is smaller than the desired max_leaf_size, then we know that the current node 
			is a leaf node (and can fit our desired number of primitives), so we can point our current node's start and end 
			member variables to the start and end of our passed in primitives respectively.
		</p>

		<p>
			If there are currently more primitives in the current node's bounding box than can fit in a leaf, then we know that we 
			have to split the current node's bounding box into 2 nodes with their respective bounding boxes. 
			Note that splitting the current node's bounding box into 2 means that the 2 resulting nodes' 
			bounding boxes will be disjoint (or not share any primitives). We chose to do this split across 
			our current node's bounding box's longest axis, which was found by comparing its dimensions and 
			sorting our primitives according to their bounding box's centroid location in said axis. We chose this heuristic 
			because it would produce shallower BVH trees and have less spatial overlap between the two resulting 
			bounding boxes. Splitting at the “middle” primitive ensures a more even distribution between the two 
			resulting bounding boxes, and less overlap between each bounding box ensures that we won't have unnecessary intersection tests. Once we determined 
			which axis was the longest, we sorted each primitive contained in the current node's bounding box by 
			its personal bounding box's centroid location in our chosen axis from smallest to largest. Now, we can 
			set our new start and end primitives accordingly along with obtaining the primitive that is in the 
			middle of the rest of our primitives (which is where the split is going to happen). Finally, we can 
			set our current node's left child to a BVH containing primitives iterating from our sorted start to 
			just before the middle primitive, and our current node's right child to a BVH containing primitives 
			iterating from the middle primitive to our sorted end.
		</p>

		<p>Here are a few images of large .dae files that are only able to be rendered with BVH acceleration.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/maxplanck_noBVH10threads.PNG" width="400px"/>
				  <figcaption>Max Planck without BVH Acceleration (Rendered in 189.8985s)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/maxplanck_withBVH10threads.PNG" width="400px"/>
				  <figcaption>Max Planck with BVH Acceleration (Rendered in 0.0985s)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBlucy_noBVH10threads.PNG" width="400px"/>
				  <figcaption>CB Lucy without BVH Acceleration (Rendered in 604.0365s)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBlucy_withBVH10threads.PNG" width="400px"/>
				  <figcaption>CB Lucy with BVH Acceleration (Rendered in 0.0646s)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBdragon_noBVH10threads.PNG" width="400px"/>
				  <figcaption>CB Dragon without BVH Acceleration (Rendered in 431.1751s)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBdragon_withBVH10threads.PNG" width="400px"/>
				  <figcaption>CB Dragon with BVH Acceleration (Rendered in 0.2378s)</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			When comparing the rendering times of these scenes with moderately complex geometries with and 
			without BVH acceleration, we can see that rendering with an acceleration structure makes a drastic 
			difference. Without implementing BVH acceleration, these scenes containing tens of thousands of 
			primitives took several minutes to fully render, while they only took less than a second post-BVH 
			acceleration implementation. Given that our implementation allows us to only analyze relevant 
			bounding boxes for path tracing, our rendering's efficiency has shot up through the roof.
		</p>

		<p>
			(Note that I rendered this on my desktop which has an NVIDIA GeForce RTX 3080 graphics card, 
			an Intel i7-12700K, and 32 GB of DDR4 RAM while allocating 10 threads.)
		</p>

		<h2>Part 3: Direct Illumination</h2>

		<p>
			When calculating direct illumination, you have to add together zero-bounce radiance and one-bounce radiance. Zero-bounce radiance is the light that directly hits the camera uninterrupted, which was pretty straightforward to implement, as all we needed to do was return the emission of the desired intersection.
		</p>
		<p>
			Then, one-bounce radiance is a little more complicated, as there are 2 ways to calculate it: uniform hemisphere sampling or importance sampling. For uniform hemisphere sampling, we use the Monte Carlo estimator in order to estimate how much light arrives at a given intersection point. So given an intersection point, we sample N number of outgoing rays (outgoing from that point) and see if any of those rays intersect a light source. If they do, then we divide it by the pdf of a uniformly sampled hemisphere (1 / (2 * PI)). Next, for importance sampling, we want to only sample directions that point between the light source and our point p. To implement this, we want to iterate through all the lights in a given scene and define the number of sampling points, or num_samples, for each light source as 1 if the light is a point light or ns_area_light if it isn’t. So, we can iterate through each light source and sample the incoming radiance coming from the current light source onto our current hit point num_samples times. Once we have our sample, we can check if the light comes from the upper hemisphere of the surface, and if it does, we can create a “shadow ray” that points from the hit point to the light and initialize its max_t to dist_to_light. From there, we can check if an object blocks this shadow ray with an intersection check, and if it isn’t blocked, then we can accumulate the direct lighting contribution for our current light source. After that, we can divide the accumulated light contribution by the number of samples taken for this light source and then add it to the total contribution coming from all light sources in the scene.
		</p>
		<p>Now, let's look at some images rendered with both implementations of the direct lighting fucntion with importance sampling on the left, and hemispherical sampling on the right! (Rendered at 16 samples per pixel and with 8 light rays)</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part31importance_1618.PNG" width="400px"/>
				  <figcaption>CBbunny rendered with importance sampling. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part31hemi_1618.PNG" width="400px"/>
				  <figcaption>CBbunny rendered with hemispherical sampling.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part31importance_1618_spheres.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with importance sampling. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part31hemi_1618_spheres.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with hemispherical sampling.</figcaption>
				</td>
			  </tr>
			
			</table>
		</div>

		<p>Next, let's compare the noise levels in soft shadows when rendering with various numbers of light rays using importance sampling. (Rendered at 1 sample-per-pixel)</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part32spheres1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with 1 light ray. </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="images/part32spheres4.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with 4 light rays. </figcaption>
				  </td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
					<img src="images/part32spheres16.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with 16 light rays. </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="images/part32spheres64.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with 64 light rays. </figcaption>
				  </td>
			  </tr>
			
			</table>
		</div>

		<p>
			Comparing the results between uniform hemisphere sampling and lighting/importance sampling, we can see that uniform sampling produces an image that is overall noisier and grainier. Since importance sampling only samples in directions that are guaranteed to be hit by the light source, a lot of the noise is eliminated when compared to the nondiscriminatory sampling involved with uniform hemisphere sampling. More specifically, uniform hemisphere sampling will sample directions that shouldn’t be contributing to the accumulation of light, which leads to a lot more noise. Thus, we should use importance sampling to achieve an image more representative of real life.
		</p>

		<h2>Part 4: Global Illumination</h2>
		<p>
			When simulating lighting, it is easier to think about it as 2 different parts: zero-bounce and at-least-one-bounce radiance. Zero-bounce radiance is the light emitted from a point p, where p is on a light source. This is the light that directly travels from a light source to a “camera” (that can we an eye or any other sensor you may use to detect light). Then, there is one-bounce radiance (or the first “bounce” involved with at-least-one-bounce radiance), which is the light that reaches the camera from a point x after the light coming from p “bounces off” the surface that x is on. The sum of these two parts of radiance is called direct lighting.
		</p>
		<p>
			As for the rest of at-least-one-bounce radiance, we refer to it as indirect lighting. Like the name suggests, it is the sum of the subsequent bounces of light that helps complete the image, as we know light just doesn’t stop after one bounce in real life. However, since light can technically bounce around for an indefinite amount of time, and we don’t want our renders to go on indefinitely, our implementation uses two parameters to help us decide when to stop calculating subsequent bounces for a given ray of light. These are p, which stands for continuation probability, and max_ray_depth. So, after a ray is cast, we make sure to add its first bounce to its radiance summation (hence the “at-least” portion of the variable name), but every subsequent bounce has a chance p of being added to the summation. For example, in our implementation, we chose p to be 0.7, which means that there is a 70% chance of adding the next bounce’s radiance contribution to the ray’s overall radiance summation. Now, can’t this technically go on forever? Well, that’s where the variable max_ray_depth comes in. The variable max_ray_depth is used as a “hard boundary” for ray depth summation, meaning that after we reach the max_ray_depth number of bounces for a given ray, we will return the calculated radiance up to that point.
		</p>
		<p>
			Now for the specifics of our indirect lighting function that takes in a Ray r and Intersection isect, we used a recursive method for our implementation. Note that every time we start calculations for a new ray (not for a ray’s subsequent bounces), it is initialized with a depth of max_ray_depth, which is important for tracking which bounce of light we are on. At the beginning, we always set the current level’s outgoing radiance (given by calling the one_bounce_radiance on the current r and isect) to a variable L_out. Then, we check if the current ray’s depth is 0, and if it is, that means that we’ve reached our max_ray_depth and should just return the current level’s outgoing radiance. If not, that means we potentially still have more bounces to accumulate recursively. At this point, we check if the current ray’s depth is equal to the max_ray_depth (which means we should continue in order to calculate the first bounce) or if we “win” the coin flip with the probability defined by variable p. If we successfully continue, then we should sample the incoming direction, outgoing direction, and pdf given our current ray and intersection using the sample_f function that we implemented in a previous part. Using these parameters (along with some conversion math) we create a sample ray that is outgoing from our current intersection point and that has a depth of the current ray’s depth minus one (this is why checking if the current ray depth is equal to 0 at the beginning of the function helps us terminate when needed). After that, we check if our new ray intersects our bvh using the intersect function, and if it does, we set a new variable Li equal to a recursive call on at_least_one_bounce_radiance with the new ray and new intersection as parameters. Now, if we want to accumulate light bounces (i.e. the variable isAccumBounces is true), then we add (f * Li * w_in.z) / (pdf * p) onto L_out, where f is the evaluated BSDF at the current intersection, w_in.z is the surface normal in object space, and pdf is the probability density function at the w_in direction. All of these extra scalars help simulate light falloff. Then, if we don’t want to accumulate light bounces (i.e. the variable isAccumBounces is false), then we just set L_out equal to (f * Li * w_in.z) / (pdf * p) instead of adding it to the previously calculated L_out. Finally, we just return L_out at the end.
		</p>

		<p>Here are some images rendered with global (direct and indirect) illumination using 1024 samples per pixel.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBspheres_lambertian_s1024_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m5_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny.</figcaption>
				</td>
			  </tr>
			  
			</table>
		</div>

		<p>Now, lets look at CBbunny while examining direct and indirect illumination rendered with 1024 samples per pixel.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_direct_s1024_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with only direct illumination.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_indirect_s1024_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with only indirect illumination.</figcaption>
				</td>
			  </tr>
			  
			</table>
		</div>

		<p>We can see how prominent the shadows are seen on the bunny in direct illumination, which shows that indirect illumination really contributes to making the shadows not look pitch black, which is reminiscent of real life! Also, we can see that the ceiling is pitch black when only looking at direct lighting. This makes sense, as direct lighting only renders up to the first bounce of light from the light source and any light that would be reflected onto the ceiling would have to travel from one surface before reaching the ceiling again (i.e after the light's first bounce).</p>

		<p>Now, let's compare the mth bounce of light with max_ray_depth set to various values, with light accumulation on the left, and non-accumulation on the right! (Rendered at 1024 samples per pixel)</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m0_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 0. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m0_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 0.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m1_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 1. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m1_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 1.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m2_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 2. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m2_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 2.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m3_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 3. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m3_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 3.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m4_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 4. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m4_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 4.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m5_l16_o1.PNG" width="400px"/>
				  <figcaption>CBbunny with accumulating light bounces until max_ray_depth 5. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m5_l16_o0.PNG" width="400px"/>
				  <figcaption>CBbunny without accumulating light bounces until max_ray_depth 5.</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>Looking at the 2nd and 3rd bounce of light, we can see that the 2nd bounce of light contributes heavily to the dispersing of light throughout the entire Cornell box, helping light reach the ceiling along with other parts of the bunny. By the time of the 3rd bounce, almost every major area of the scene has been "covered" with light, so this subsequent bounce helps fill the nooks and crannies of the scene that hasn't been hit by light yet. These, along with subsequent bounces of light, help create a feeling of depth on the bunny by casting varying shadows depending on where you are viewing the bunny from. When looking at rasterization techniques that we've used in the past, techniques like supersampling basically treat the scene as a 2-dimensional plane when taking the average of surrounding pixel colors, so things like overhangs (or other objects that cast irregular shadows) would not be represented well. However, using the lighting scheme that we implemented where is physics-based, so it could account for the irregular shadows cast by irregular objects.</p>

		<p>Now, let's look at Russian Roulette rendering with max_ray_depth set to various values! (Rendered at 1024 samples per pixel)</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m0_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 0. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m1_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 1.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m2_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 2. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m3_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 3.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m4_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 4. </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_s1024_m100_l16_o1_RR.PNG" width="400px"/>
				  <figcaption>CBbunny with Russian Roulette rendering and max_ray_depth set to 100.</figcaption>
				</td>
			  </tr>
			  
			</table>
		</div>

		<p>We can see that the larger and larger max_ray_depth gets, there is less of a change in the final rendered image. This is because the program starts to terminate additional ray calculations through the previously defined "coin flip" rather than making all the way to the defined max_ray_depth.</p>

		<p>Now, let's look at CBspheres_lambertian with various sample-per-pixel rates! (Rendered with 4 light rays)</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBspheres_lambertian_s1_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 1. </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="images/CBspheres_lambertian_s2_m5_l4_o1.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 2. </figcaption>
				  </td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBspheres_lambertian_s4_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 4. </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="images/CBspheres_lambertian_s8_m5_l4_o1.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 8. </figcaption>
				  </td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBspheres_lambertian_s16_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 16. </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="images/CBspheres_lambertian_s64_m5_l4_o1.PNG" width="400px"/>
					<figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 64. </figcaption>
				  </td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBspheres_lambertian_s1024_m5_l4_o1.PNG" width="400px"/>
				  <figcaption>CBspheres_lambertian rendered with a sample-per-pixel rate of 1024. </figcaption>
				</td>
				
			  </tr>
			  
			</table>
		</div>

		<p>We can see that as we increase the sample-per-pixel rate, the level of noise and grain decreases when rendering these images!</p>


		<h2>Part 5: Adaptive Sampling</h2>
		Adaptive sampling is a technique in rendering where the number of samples taken per pixel is dynamically adjusted based on how much the samples vary. Instead of taking a fixed number of samples for each pixel, adaptive sampling continues until the estimated pixel color has low enough variance — meaning additional samples likely won't change the result much.

		In the adaptive sampling implementation of <code>PathTracer::raytrace_pixel()</code>, the function dynamically determines how many samples to take per pixel based on statistical confidence. For each sample, it generates a subpixel coordinate using a sampler, constructs a ray through the camera, and estimates radiance via global illumination. It accumulates both the radiance and the illuminance (brightness) for variance analysis. Every samplesPerBatch samples, it computes the mean and variance of the illuminance and uses these to calculate a 95% confidence interval. If this interval is smaller than maxTolerance times the mean brightness, it concludes that further sampling would not significantly change the pixel value and terminates early. This balances rendering quality and performance by avoiding unnecessary samples in low-variance regions while ensuring accuracy in complex areas.

		<p>
			Here are 2 scenes rendered with at least 2048 samples per pixel along with their sampling rate images.
		</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part5bunny.png" width="400px"/>
				  <figcaption>CBunny.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part5bunny_rate.png" width="400px"/>
				  <figcaption>CBunny Sampling Rate.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/part5spheres.png" width="400px"/>
				  <figcaption>CBspheres_lambertian.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/part5spheres_rate.png" width="400px"/>
				  <figcaption>CBspheres_lambertian Sampling Rate.</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		
		</div>
	</body>
</html>